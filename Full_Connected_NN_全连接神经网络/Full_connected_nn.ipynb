{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab only!\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-5-2e82a26757ae>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "MINST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size=200\n",
    "learning_rate=0.01\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "(x, y),(x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "def preprocess(x, y):\n",
    "  x = (tf.cast(x, tf.float32)/255)-0.1307\n",
    "  y = tf.cast(y, tf.int32)\n",
    "#   y = tf.one_hot(y,depth=10)   \n",
    "  return x, y\n",
    "\n",
    "ds_train = ds_train.map(preprocess).shuffle(1000).batch(batch_size)\n",
    "ds_test = ds_train.map(preprocess).shuffle(1000).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 28, 28) (200, 10)\n"
     ]
    }
   ],
   "source": [
    "type(ds_train)\n",
    "image, label = next(iter(ds_train))\n",
    "print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 28, 28]) torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "type(train_loader)\n",
    "[image, label] = next(iter(train_loader))\n",
    "print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-connnected manually\n",
    "[b, 786] -> [b, 200] -> [b, 100] -> [b, 10]\n",
    "\n",
    "w1: [786, 200], b1: [200],\n",
    "\n",
    "w2: [200,100], b2: [100], \n",
    "\n",
    "w3: [100,10], b3ï¼š[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, step:0 loss:1404.9921875\n",
      "epoch:0, step:100 loss:2.2279036045074463\n",
      "epoch:0, step:200 loss:2.3840603828430176\n",
      "epoch:1, step:0 loss:2.1420228481292725\n",
      "epoch:1, step:100 loss:2.0314340591430664\n",
      "epoch:1, step:200 loss:2.0380284786224365\n",
      "epoch:2, step:0 loss:2.127847671508789\n",
      "epoch:2, step:100 loss:2.021336793899536\n",
      "epoch:2, step:200 loss:1.6273704767227173\n",
      "epoch:3, step:0 loss:1.5105515718460083\n",
      "epoch:3, step:100 loss:1.5376299619674683\n",
      "epoch:3, step:200 loss:1.2730509042739868\n",
      "epoch:4, step:0 loss:1.2069252729415894\n",
      "epoch:4, step:100 loss:1.358099341392517\n",
      "epoch:4, step:200 loss:1.1399234533309937\n",
      "epoch:5, step:0 loss:1.064672827720642\n",
      "epoch:5, step:100 loss:1.315996527671814\n",
      "epoch:5, step:200 loss:1.071283221244812\n",
      "epoch:6, step:0 loss:1.001338243484497\n",
      "epoch:6, step:100 loss:0.7731111645698547\n",
      "epoch:6, step:200 loss:0.4953363537788391\n",
      "epoch:7, step:0 loss:0.6500086784362793\n",
      "epoch:7, step:100 loss:0.7191830277442932\n",
      "epoch:7, step:200 loss:0.5285062193870544\n",
      "epoch:8, step:0 loss:0.4401146173477173\n",
      "epoch:8, step:100 loss:0.500275731086731\n",
      "epoch:8, step:200 loss:0.3191242516040802\n",
      "epoch:9, step:0 loss:0.46588724851608276\n",
      "epoch:9, step:100 loss:0.5581982731819153\n",
      "epoch:9, step:200 loss:0.4840467870235443\n"
     ]
    }
   ],
   "source": [
    "# weights and bias\n",
    "w1 = tf.Variable(tf.random.uniform([28*28, 200]))\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "\n",
    "w2 = tf.Variable(tf.random.uniform([200, 100]))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "w3 = tf.Variable(tf.random.uniform([100, 10]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\n",
    "# forward func\n",
    "def model(x):\n",
    "    x = tf.nn.relu(x@w1 + b1)\n",
    "    x = tf.nn.relu(x@w2 + b2)\n",
    "    x = x@w3 + b3\n",
    "        \n",
    "    return x\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for step, (x, y) in enumerate(ds_train):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        with tf.GradientTape() as tape:            \n",
    "            logits = model(x)\n",
    "            \n",
    "            losses = tf.losses.sparse_categorical_crossentropy(y,logits,from_logits=True)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        grads = tape.gradient(loss, [w1,b1,w2,b2,w3,b3])\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, [w1,b1,w2,b2,w3,b3]))\n",
    "        \n",
    "        if(step%100==0):\n",
    "            print(\"epoch:{}, step:{} loss:{}\".\n",
    "                  format(epoch, step, loss.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, step:0, loss:9682.416015625\n",
      "epoch:0, step:100, loss:2.488150119781494\n",
      "epoch:0, step:200, loss:2.2699265480041504\n",
      "epoch:1, step:0, loss:2.274050235748291\n",
      "epoch:1, step:100, loss:2.34322452545166\n",
      "epoch:1, step:200, loss:2.2167744636535645\n",
      "epoch:2, step:0, loss:2.3210036754608154\n",
      "epoch:2, step:100, loss:2.2207939624786377\n",
      "epoch:2, step:200, loss:2.4887542724609375\n",
      "epoch:3, step:0, loss:2.344801187515259\n",
      "epoch:3, step:100, loss:2.1573853492736816\n",
      "epoch:3, step:200, loss:2.2278380393981934\n",
      "epoch:4, step:0, loss:2.2109551429748535\n",
      "epoch:4, step:100, loss:2.190887928009033\n",
      "epoch:4, step:200, loss:2.702714204788208\n",
      "epoch:5, step:0, loss:2.1471283435821533\n",
      "epoch:5, step:100, loss:2.527514696121216\n",
      "epoch:5, step:200, loss:2.131390333175659\n",
      "epoch:6, step:0, loss:2.4100725650787354\n",
      "epoch:6, step:100, loss:1.98695969581604\n",
      "epoch:6, step:200, loss:1.888046383857727\n",
      "epoch:7, step:0, loss:2.0443994998931885\n",
      "epoch:7, step:100, loss:1.8937968015670776\n",
      "epoch:7, step:200, loss:1.9226715564727783\n",
      "epoch:8, step:0, loss:1.7935198545455933\n",
      "epoch:8, step:100, loss:1.7859866619110107\n",
      "epoch:8, step:200, loss:1.7676191329956055\n",
      "epoch:9, step:0, loss:1.6434701681137085\n",
      "epoch:9, step:100, loss:1.6301932334899902\n",
      "epoch:9, step:200, loss:1.8712739944458008\n"
     ]
    }
   ],
   "source": [
    "# weights and bias\n",
    "w1 = torch.rand(28*28, 200 , requires_grad=True)\n",
    "b1 = torch.zeros(200, requires_grad=True)\n",
    "\n",
    "w2 = torch.rand(200, 100, requires_grad=True)\n",
    "b2 = torch.zeros(100, requires_grad=True)\n",
    "\n",
    "w3 = torch.rand(100, 10, requires_grad=True)\n",
    "b3 = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "# torch.nn.init.kaiming_normal_(w1)\n",
    "# torch.nn.init.kaiming_normal_(w2)\n",
    "# torch.nn.init.kaiming_normal_(w3)\n",
    "\n",
    "# forward func\n",
    "def forward(x):\n",
    "    x = F.relu(x@w1 + b1)\n",
    "    x = F.relu(x@w2 + b2)\n",
    "    x = x@w3 + b3\n",
    "        \n",
    "    return x\n",
    "\n",
    "optimizer = torch.optim.Adam([w1,b1,w2,b2,w3,b3],\n",
    "                            lr=learning_rate)\n",
    "criteon = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        \n",
    "        logits = forward(x)\n",
    "        loss = criteon(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(step%100 == 0):\n",
    "            print(\"epoch:{}, step:{}, loss:{}\".\n",
    "                  format(epoch, step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-connnected higher level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, step:0 loss:2.3339242935180664\n",
      "epoch:0, step:100 loss:0.16767878830432892\n",
      "epoch:0, step:200 loss:0.2757965922355652\n",
      "epoch:1, step:0 loss:0.09909996390342712\n",
      "epoch:1, step:100 loss:0.21857158839702606\n",
      "epoch:1, step:200 loss:0.06962249428033829\n",
      "epoch:2, step:0 loss:0.08726327121257782\n",
      "epoch:2, step:100 loss:0.13604803383350372\n",
      "epoch:2, step:200 loss:0.15911605954170227\n",
      "epoch:3, step:0 loss:0.14188748598098755\n",
      "epoch:3, step:100 loss:0.09770938754081726\n",
      "epoch:3, step:200 loss:0.07950416207313538\n",
      "epoch:4, step:0 loss:0.07333747297525406\n",
      "epoch:4, step:100 loss:0.0721074566245079\n",
      "epoch:4, step:200 loss:0.06202578917145729\n",
      "epoch:5, step:0 loss:0.09106221795082092\n",
      "epoch:5, step:100 loss:0.035374075174331665\n",
      "epoch:5, step:200 loss:0.04066429287195206\n",
      "epoch:6, step:0 loss:0.06491152942180634\n",
      "epoch:6, step:100 loss:0.06632785499095917\n",
      "epoch:6, step:200 loss:0.02190435864031315\n",
      "epoch:7, step:0 loss:0.07481320202350616\n",
      "epoch:7, step:100 loss:0.035911109298467636\n",
      "epoch:7, step:200 loss:0.027494270354509354\n",
      "epoch:8, step:0 loss:0.0957433208823204\n",
      "epoch:8, step:100 loss:0.040801677852869034\n",
      "epoch:8, step:200 loss:0.05833094194531441\n",
      "epoch:9, step:0 loss:0.1194169744849205\n",
      "epoch:9, step:100 loss:0.03284149616956711\n",
      "epoch:9, step:200 loss:0.10545062273740768\n"
     ]
    }
   ],
   "source": [
    "class FC_model(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.model = keras.Sequential(\n",
    "            [layers.Dense(200),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(100),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(10)]\n",
    "            )\n",
    "    \n",
    "    def call(self,x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = FC_model()\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for step, (x, y) in enumerate(ds_train):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        with tf.GradientTape() as tape:            \n",
    "            logits = model(x)\n",
    "            \n",
    "            losses = tf.losses.sparse_categorical_crossentropy(y,logits,from_logits=True)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        \n",
    "        if(step%100==0):\n",
    "            print(\"epoch:{}, step:{} loss:{}\".\n",
    "                  format(epoch, step, loss.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, step:0, loss:2.3125956058502197\n",
      "epoch:0, step:100, loss:0.22973255813121796\n",
      "epoch:0, step:200, loss:0.1388895958662033\n"
     ]
    }
   ],
   "source": [
    "class FC_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100,10)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "network = FC_NN().to(device)        \n",
    "optimizer = torch.optim.Adam(network.parameters(),\n",
    "                            lr=learning_rate)\n",
    "criteon = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        logits = network(x)\n",
    "        loss = criteon(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(step%100 == 0):\n",
    "            print(\"epoch:{}, step:{}, loss:{}\".\n",
    "                  format(epoch, step, loss.item()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab only!\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-3-2e82a26757ae>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label, depth):\n",
    "    out = torch.zeros(label.size(0), depth)\n",
    "    idx = torch.LongTensor(label).view(-1, 1)\n",
    "    out.scatter_(dim=1, index=idx, value=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.23290308, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.00120118 -0.00120119]\n",
      " [ 0.01929211 -0.01929212]\n",
      " [ 0.03523264 -0.03523265]\n",
      " [ 0.04114018 -0.04114018]], shape=(4, 2), dtype=float32)\n",
      "tf.Tensor([ 0.07563752 -0.07563753], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = tf.random.uniform([3,4])\n",
    "w = tf.random.uniform([4,2])\n",
    "b = tf.zeros([2])\n",
    "y = tf.constant([0, 1, 1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # if the tensors are not variables\n",
    "    tape.watch([w,b])\n",
    "    \n",
    "    logits = x @ w + b\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    y_true = tf.one_hot(y, depth=2)\n",
    "    \n",
    "    losses = tf.losses.MSE(y_true,probs)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    \n",
    "grads = tape.gradient(loss, [w,b])\n",
    "\n",
    "grads_w = grads[0]\n",
    "grads_b = grads[1]\n",
    "\n",
    "print(loss)\n",
    "print(grads[0])\n",
    "print(grads[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1932, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.0028,  0.0028],\n",
      "        [-0.0552,  0.0552],\n",
      "        [ 0.0329, -0.0329],\n",
      "        [-0.0147,  0.0147]])\n",
      "tensor([-0.0148,  0.0148])\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "w = torch.rand([4,2], requires_grad=True)\n",
    "b = torch.zeros([2], requires_grad=True)\n",
    "y = torch.LongTensor([0, 1, 1])\n",
    "\n",
    "# if \"requires_grad=Flase\"\n",
    "# w.requires_grad_()\n",
    "# b.requires_grad_()\n",
    "\n",
    "logits = x @ w +b\n",
    "probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "y_true = one_hot(y, depth=2)\n",
    "loss = F.mse_loss(y_true, probs)\n",
    "\n",
    "\n",
    "\n",
    "grads = torch.autograd.grad(loss, [w, b])\n",
    "\n",
    "grads_w = grads[0]\n",
    "grads_b = grads[1]\n",
    "\n",
    "\n",
    "print(loss)\n",
    "print(grads_w)\n",
    "print(grads_b)\n",
    "\n",
    "# Alternative way:\n",
    "\n",
    "# loss.backward()\n",
    "# print(w.grad)\n",
    "# print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[2.6201749e-01, 9.3732166e-01, 1.5761006e-01],\n",
      "       [6.7006159e-01, 6.8807602e-04, 7.3578167e-01],\n",
      "       [6.9119418e-01, 9.5617390e-01, 3.1275153e-02]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[0.2586995  0.5082489  0.23305157]\n",
      " [0.38760337 0.19846426 0.41393238]\n",
      " [0.3545725  0.4621514  0.1832761 ]], shape=(3, 3), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "logits = tf.random.uniform([3,3])\n",
    "logits = tf.Variable(logits)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "#     tape.watch([logits])\n",
    "    \n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    \n",
    "grads = tape.gradient(probs[1][1], logits)\n",
    "\n",
    "\n",
    "# print(logits)\n",
    "# print(probs)\n",
    "\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5588, 0.8455, 0.1835],\n",
      "        [0.2511, 0.7790, 0.7572],\n",
      "        [0.5120, 0.8886, 0.9761]], requires_grad=True)\n",
      "tensor([[0.3312, 0.4412, 0.2276],\n",
      "        [0.2297, 0.3894, 0.3810],\n",
      "        [0.2470, 0.3600, 0.3930]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0894,  0.2378, -0.1483],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# logist: [b, 3], probs: [b, 3]\n",
    "logits = torch.rand(3,3)\n",
    "logits.requires_grad_()\n",
    "\n",
    "probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "print(logits)\n",
    "print(probs)\n",
    "\n",
    "probs[1][1].backward()\n",
    "print(logits.grad)\n",
    "\n",
    "\n",
    "# grad_0_0 = torch.autograd.grad(probs[0][0], logits, retain_graph=True)\n",
    "# print(grad_0_0)\n",
    "\n",
    "# grad_1_1 = torch.autograd.grad(probs[1][1], logits, retain_graph=True)\n",
    "# print(grad_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda46a38bc10b2f4a48a71d1cc22a9c8986"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

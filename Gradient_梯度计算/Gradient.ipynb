{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab only!\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-3-2e82a26757ae>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label, depth):\n",
    "    out = torch.zeros(label.size(0), depth)\n",
    "    idx = torch.LongTensor(label).view(-1, 1)\n",
    "    out.scatter_(dim=1, index=idx, value=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0189726  -0.01897261]\n",
      " [-0.02627433  0.02627433]\n",
      " [-0.02474528  0.02474528]\n",
      " [-0.05913349  0.05913348]], shape=(4, 2), dtype=float32)\n",
      "tf.Tensor([ 0.05598413 -0.05598414], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.5465539  1.8689929 ]\n",
      " [0.6153563  0.8887136 ]\n",
      " [0.72412205 0.8128369 ]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.42008144 0.57991856]\n",
      " [0.4320831  0.5679169 ]\n",
      " [0.4778358  0.52216417]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = tf.random.uniform([3,4])\n",
    "w = tf.Variable(tf.random.uniform([4,2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "y = tf.constant([0, 1, 1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "#     tape.watch([w,b])\n",
    "    logits = x @ w + b\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    y_true = tf.one_hot(y, depth=2)\n",
    "    \n",
    "    losses = tf.losses.MSE(y_true,probs)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    \n",
    "grads = tape.gradient(loss, [w,b])\n",
    "\n",
    "grads_w = grads[0]\n",
    "grads_b = grads[1]\n",
    "\n",
    "print(grads[0])\n",
    "print(grads[1])\n",
    "\n",
    "print(logits)\n",
    "print(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3485, grad_fn=<MeanBackward0>)\n",
      "tensor([[ 0.0356, -0.0356],\n",
      "        [ 0.0069, -0.0069],\n",
      "        [ 0.1585, -0.1585],\n",
      "        [ 0.0573, -0.0573]])\n",
      "tensor([ 0.1570, -0.1570])\n",
      "tensor([[1.4062, 0.4471],\n",
      "        [1.4924, 0.4679],\n",
      "        [0.9863, 0.3508]], grad_fn=<AddBackward0>)\n",
      "tensor([[0.7229, 0.2771],\n",
      "        [0.7359, 0.2641],\n",
      "        [0.6537, 0.3463]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "w = torch.rand([4,2], requires_grad=True)\n",
    "b = torch.zeros([2], requires_grad=True)\n",
    "y = torch.LongTensor([0, 1, 1])\n",
    "\n",
    "logits = x @ w +b\n",
    "probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "y_true = one_hot(y, depth=2)\n",
    "loss = F.mse_loss(y_true, probs)\n",
    "# grads = torch.autograd.grad(loss, [w, b])\n",
    "\n",
    "# grads_w = grads[0]\n",
    "# grads_b = grads[1]\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(loss)\n",
    "# print(grads[0])\n",
    "# print(grads[1])\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "print(logits)\n",
    "print(probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda46a38bc10b2f4a48a71d1cc22a9c8986"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

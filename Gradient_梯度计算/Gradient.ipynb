{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab only!\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-3-2e82a26757ae>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label, depth):\n",
    "    out = torch.zeros(label.size(0), depth)\n",
    "    idx = torch.LongTensor(label).view(-1, 1)\n",
    "    out.scatter_(dim=1, index=idx, value=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.23290308, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.00120118 -0.00120119]\n",
      " [ 0.01929211 -0.01929212]\n",
      " [ 0.03523264 -0.03523265]\n",
      " [ 0.04114018 -0.04114018]], shape=(4, 2), dtype=float32)\n",
      "tf.Tensor([ 0.07563752 -0.07563753], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = tf.random.uniform([3,4])\n",
    "w = tf.random.uniform([4,2])\n",
    "b = tf.zeros([2])\n",
    "y = tf.constant([0, 1, 1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # if the tensors are not variables\n",
    "    tape.watch([w,b])\n",
    "    \n",
    "    logits = x @ w + b\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    y_true = tf.one_hot(y, depth=2)\n",
    "    \n",
    "    losses = tf.losses.MSE(y_true,probs)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    \n",
    "grads = tape.gradient(loss, [w,b])\n",
    "\n",
    "grads_w = grads[0]\n",
    "grads_b = grads[1]\n",
    "\n",
    "print(loss)\n",
    "print(grads[0])\n",
    "print(grads[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2377, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.0021,  0.0021],\n",
      "        [ 0.0981, -0.0981],\n",
      "        [ 0.0548, -0.0548],\n",
      "        [ 0.0253, -0.0253]])\n",
      "tensor([ 0.0360, -0.0360])\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "w = torch.rand([4,2], requires_grad=True)\n",
    "b = torch.zeros([2], requires_grad=True)\n",
    "y = torch.LongTensor([0, 1, 1])\n",
    "\n",
    "# if \"requires_grad=Flase\"\n",
    "# w.requires_grad_()\n",
    "# b.requires_grad_()\n",
    "\n",
    "logits = x @ w +b\n",
    "probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "y_true = one_hot(y, depth=2)\n",
    "loss = F.mse_loss(y_true, probs)\n",
    "\n",
    "\n",
    "\n",
    "grads = torch.autograd.grad(loss, [w, b])\n",
    "\n",
    "grads_w = grads[0]\n",
    "grads_b = grads[1]\n",
    "\n",
    "\n",
    "print(loss)\n",
    "print(grads_w)\n",
    "print(grads_b)\n",
    "\n",
    "# Alternative way:\n",
    "\n",
    "# loss.backward()\n",
    "# print(w.grad)\n",
    "# print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.29163083, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.7382552e-08 1.6526526e-08 2.5695563e-08]\n",
      " [1.5146206e-08 1.5726819e-08 2.8731616e-08]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "logits = tf.random.uniform([3,3])\n",
    "logits = tf.Variable(logits)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "#     tape.watch([logits])\n",
    "    \n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    \n",
    "grads = tape.gradient(probs, logits)\n",
    "\n",
    "\n",
    "# print(logits)\n",
    "# print(probs)\n",
    "\n",
    "print(probs[0][0])\n",
    "print(grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3427, 0.7886, 0.0762],\n",
      "        [0.9033, 0.4702, 0.1778],\n",
      "        [0.4810, 0.8037, 0.6519]], requires_grad=True)\n",
      "tensor([[0.3005, 0.4693, 0.2302],\n",
      "        [0.4689, 0.3041, 0.2270],\n",
      "        [0.2803, 0.3871, 0.3326]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.2102, -0.1410, -0.0692],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# logist: [b, 3], probs: [b, 3]\n",
    "logits = torch.rand(3,3)\n",
    "logits.requires_grad_()\n",
    "\n",
    "probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "print(logits)\n",
    "print(probs)\n",
    "\n",
    "\n",
    "probs[0][0].backward(retain_graph=True)\n",
    "print(logits.grad)\n",
    "\n",
    "# probs[1][1].backward(retain_graph=True)\n",
    "# print(logits.grad)\n",
    "\n",
    "# probs[2][2].backward(retain_graph=True)\n",
    "# print(logits.grad)\n",
    "\n",
    "# probs[0][1].backward(retain_graph=True)\n",
    "# print(logits.grad)\n",
    "\n",
    "\n",
    "\n",
    "# grad_0_0 = torch.autograd.grad(probs[0][0], logits, retain_graph=True)\n",
    "# print(grad_0_0)\n",
    "\n",
    "# grad_1_1 = torch.autograd.grad(probs[1][1], logits, retain_graph=True)\n",
    "# print(grad_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.6436507, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.12311882 -0.12311883]\n",
      " [ 0.11273158 -0.11273161]\n",
      " [ 0.17509589 -0.1750959 ]\n",
      " [ 0.17360634 -0.17360635]], shape=(4, 2), dtype=float32)\n",
      "tf.Tensor([ 0.10200089 -0.10200092], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot depth = 2\n",
    "\n",
    "x = tf.random.uniform([3,4])\n",
    "w = tf.random.uniform([4,2])\n",
    "b = tf.zeros([2])\n",
    "y = tf.constant([0, 1, 1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w,b])\n",
    "    y_true = tf.one_hot(y, depth=2)\n",
    "    logits = x@w + b\n",
    "    losses = tf.losses.categorical_crossentropy(y_true, \n",
    "                                                logits, \n",
    "                                                from_logits=True)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    \n",
    "grads = tape.gradient(loss, [w,b])  \n",
    "\n",
    "grad_w = grads[0]\n",
    "grad_b = grads[1]\n",
    "\n",
    "print(loss)\n",
    "print(grad_w)\n",
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'backword'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d774f1b5b365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'backword'"
     ]
    }
   ],
   "source": [
    "# Example: [3,4] linear conversion ->[3,2]\n",
    "#  y = x*w +c  x:[3,4] w:[4,2] b:[2], y:[3]\n",
    "#  y one-hot not requried here\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "w = torch.rand(4,2, requires_grad=True)\n",
    "b = torch.zeros(2, requires_grad=True)\n",
    "y = torch.LongTensor([0, 1, 1])\n",
    "\n",
    "logits = x@w + b\n",
    "# must use logits rather than probs, one-hot encoding not required\n",
    "loss = F.cross_entropy(logits, y)\n",
    "\n",
    "loss.bac()\n",
    "\n",
    "print(loss)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
